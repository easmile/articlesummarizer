{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Extractive Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTjVYxuW-Sri"
      },
      "source": [
        "##Extractive Model\n",
        "This completed notebook was provided by Mehul Gupta and can be found here: https://medium.com/data-science-in-your-pocket/text-summarization-using-textrank-in-nlp-4bce52c5b390\n",
        "The model has been adapted to manually take as input an amazon review and has had the output length changed to one sentence only.\n",
        "\n",
        "This is the extractive model used to produce summaries and utilises the TextRank algorithm. The library Networkx provided the PageRank algorithm which the TextRank algorithm is based on. The PageRank algorithm is implemented in this model by calling the function nx.pagerank_numpy()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I80fiYoB_hrW"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaLAT6CB3ACN",
        "outputId": "1f89a0d9-5443-4fd1-8948-484f6d67be5a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from gensim.models import Word2Vec\n",
        "from scipy import spatial\n",
        "import networkx as nx"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD0lDL2D_xmP"
      },
      "source": [
        "The review variable holds the original review that will be used to produce a summary. This variable must be updated manually to produce a different summary. Below are some example reviews taken from the amazon dataset that can be used:\n",
        "\n",
        "'For me: too sweet. Its like eating candied meat. Not the savory item I was hoping for! Id skip this option and go for original.'\n",
        "Gave me such a caffeine overdose I had the shakes, a racing heart and an anxiety attack. Plus it tastes unbelievably bad. I'll stick with coffee, tea and soda, thanks.                                                   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VehQUFL33Gnn"
      },
      "source": [
        "review = 'Maybe I got the runt of the litter, but when I opened the bag, the foul musty/moldy smell hit me in the face. Even though the label (which was just a printed sticker as opposed to something stamped on the bag itself) said good until 1/2013, there was no doubt that the food had gone bad. Very disappointing.'                                                   "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g8znD_kA9qH"
      },
      "source": [
        "### Tokenise review into a list of sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rToZXNsb3Weu"
      },
      "source": [
        "sentences=sent_tokenize(review)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSMrnDtABQd8"
      },
      "source": [
        "###Preprocess Data\n",
        "This cleans the review by performing tasks such as removing punctuation marks, special characters, and removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl18FNK83Zw2"
      },
      "source": [
        "sentences_clean=[re.sub(r'[^\\w\\s]','',sentence.lower()) for sentence in sentences]\n",
        "stop_words = stopwords.words('english')\n",
        "sentence_tokens=[[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_clean]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzHA8EfEB6k3"
      },
      "source": [
        "### Calculate sentence embeddings\n",
        "The library gensim is used to calculated the embedding for each word in the review. These are then used to calculate sentence embeddings which are then padded with 0s to ensure all embedded sentences are the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BGO30CL3d0O",
        "outputId": "9b5efad8-9a0f-4d5a-fa8f-1d4f94cd6716"
      },
      "source": [
        "w2v=Word2Vec(sentence_tokens,size=1,min_count=1,iter=100)\n",
        "sentence_embeddings=[[w2v[word][0] for word in words] for words in sentence_tokens]\n",
        "max_len=max([len(tokens) for tokens in sentence_tokens])\n",
        "sentence_embeddings=[np.pad(embedding,(0,max_len-len(embedding)),'constant') for embedding in sentence_embeddings]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jPHtH4BC9y7"
      },
      "source": [
        "### Calculate similarity matrix\n",
        "An initial similarity matrix is created that is filled with 0s. The cosine distance is used to calculate the similarity between sentence pairs in the review. This value is then used to update the matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPIfLZO-31Nu"
      },
      "source": [
        "similarity_matrix = np.zeros([len(sentence_tokens), len(sentence_tokens)])\n",
        "for i,row_embedding in enumerate(sentence_embeddings):\n",
        "    for j,column_embedding in enumerate(sentence_embeddings):\n",
        "        similarity_matrix[i][j]=1-spatial.distance.cosine(row_embedding,column_embedding)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov1Mx-qIDyqK"
      },
      "source": [
        "### Implement PageRank\n",
        "TextRank is based on the PageRank algorithm where instead of sentences are used instead of webpages. The Networkx implementation of PageRank is called using the similarity matrix, which has been converted to a graph, as the parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XHhaOa3353B"
      },
      "source": [
        "nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "scores = nx.pagerank_numpy(nx_graph)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRAL0yblFVY8"
      },
      "source": [
        "### Create sentence dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyczpXKw5Jkz"
      },
      "source": [
        "top_sentence={sentence:scores[index] for index,sentence in enumerate(sentences)}\n",
        "top=dict(sorted(top_sentence.items(), key=lambda x: x[1], reverse=True)[:1]) #Change last value to be desired number of sentences in summary "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWvAYoYJFemq"
      },
      "source": [
        "### Produce Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FewXaCAa5NPS",
        "outputId": "62983f47-6734-4683-bdcc-24da41fd26e7"
      },
      "source": [
        "for sent in sentences:\n",
        "    if sent in top.keys():\n",
        "        print(sent)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maybe I got the runt of the litter, but when I opened the bag, the foul musty/moldy smell hit me in the face.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}